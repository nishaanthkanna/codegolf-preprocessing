{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Parse the XML file and get the root element\n",
    "# extract all the attributes from the XML, some rows have incomplete attribute \n",
    "# list , so need to scan all rows\n",
    "def read_large_xml(filename):\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "    attr = set()\n",
    "    for i in root:\n",
    "      for j in i.attrib:\n",
    "        attr.add(j)\n",
    "    attr = list(attr)\n",
    "    # using iterparse to parse large XML files effciently\n",
    "    return pd.read_xml(filename, iterparse={\"row\": attr})\n",
    "\n",
    "posts_df = read_large_xml(\"Posts.xml\")\n",
    "comments_df = read_large_xml(\"Comments.xml\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating Parent Post (Questions) and Answers (programs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract parent posts\n",
    "parent_posts_df = posts_df[posts_df[\"ParentId\"].isnull()]\n",
    "# extract answers\n",
    "answer_posts_df = posts_df[~posts_df[\"ParentId\"].isnull()]\n",
    "# need only python answers so filtering for Python in the answer body, Most usual format for Code Golf\n",
    "python_posts_df = answer_posts_df[answer_posts_df[\"Body\"].str.contains(\"Python\")].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleantext\n",
    "from bs4 import BeautifulSoup\n",
    "# using cleantext and beautiful soup to remove HTML tags, links and other special characters from answer body to use it as a prompt for LLMs\n",
    "p_pattern = r'(?s)<p>(.*?)</p>'\n",
    "code_pattern = r'(?s)<code>(.*?)</code>'\n",
    "python_posts_df[\"Text\"] = python_posts_df[\"Body\"].str.extract(p_pattern)[0].str.replace(code_pattern, '')\n",
    "python_posts_df[\"Text\"] = python_posts_df[\"Text\"].apply(lambda x: cleantext.clean(BeautifulSoup(x if x is not np.nan else \"\", 'html.parser').get_text(), no_urls=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_long_strings(series):\n",
    "    long_strings = series.str.findall(r'\\w{11,}').explode().dropna()\n",
    "    if long_strings.empty:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return ','.join(long_strings)\n",
    "    \n",
    "# extract the python code\n",
    "# there are multiple <code></code> tags with bits of code here and there, we ignore code of length less than 10 (most likely explaining the following code).\n",
    "code_pattern = r'(?s)<code>(.*?)</code>'\n",
    "python_posts_df[\"Code\"] = python_posts_df[\"Body\"].str.extractall(code_pattern).groupby(level=0)[0].apply(lambda x: ','.join(x[x.str.len() > 10]) if not x.isnull().all() else np.nan)\n",
    "# still some code is empty because the body tag does not contain code, dropping those rows # around 200\n",
    "python_posts_df.dropna(subset=[\"Code\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the Bytes, we could extract bytes from body, but some didnt have it\n",
    "# the usage of calculating bytes is so that we could produce git diff between a code that is large and code that is small (essence of Code Golf) for the same problem\n",
    "bytes = []\n",
    "for i in python_posts_df['Code']:\n",
    "    bytes.append(len(i))\n",
    "python_posts_df[\"ByteCount\"] = bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep programs between 10 and 1000, because more than 1000, its usually text written inside the <code></code> \n",
    "python_posts_df = python_posts_df[(python_posts_df[\"ByteCount\"]>10)&(python_posts_df[\"ByteCount\"]<1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing username mentioned in comments\n",
    "def clean_text(string):\n",
    "    try:\n",
    "        return cleantext.clean(string if string is not np.nan else \"\", no_urls=True)\n",
    "    except Exception:\n",
    "        print(string)\n",
    "comments_df[\"Cleaned_Text\"] = comments_df[\"Text\"].str.replace(r'@\\w+\\s?', '').apply(clean_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating PostId:Comment mapping to later integrate with the main DataFrame and query a LLM to generate the commit message\n",
    "postid_mapping = {}\n",
    "for postid, group in comments_df.groupby([\"PostId\"]):\n",
    "    postid_mapping[postid] = \"\\n\".join(group[\"Cleaned_Text\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the comments using the dict generated above\n",
    "python_posts_df[\"Comments\"] = python_posts_df[\"Id\"].map(postid_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the dataframe, drop unnecesary columns\n",
    "python_posts_df.drop(labels=['index', 'LastActivityDate', 'LastEditDate', 'ContentLicense',\n",
    "       'OwnerUserId', 'ViewCount', 'CreationDate', 'AnswerCount', 'PostTypeId',\n",
    "       'CommentCount', 'LastEditorDisplayName', 'ClosedDate', 'Score',\n",
    "       'AcceptedAnswerId', 'LastEditorUserId', 'Title', 'Tags',\n",
    "       'OwnerDisplayName', 'CommunityOwnedDate', 'FavoriteCount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "# Generate diffs using the unified diff format\n",
    "_no_eol = \"\\ No newline at end of file\"\n",
    "def generate_diffs(row):\n",
    "    import difflib\n",
    "    # generating diffs between the larger code and smaller code\n",
    "    diffs = difflib.unified_diff(row[\"Code\"].splitlines(True),row[\"Code_shifted\"].splitlines(True),n=0)\n",
    "    try: _,_ = next(diffs),next(diffs)\n",
    "    except StopIteration: pass\n",
    "    # adding the _no_eol to end of file if code didnt have \\n character at end\n",
    "    return ''.join([d if d[-1] == '\\n' else d+'\\n'+_no_eol+'\\n' for d in diffs])\n",
    "\n",
    "diff_df = pd.DataFrame()\n",
    "\n",
    "for parentId, group in python_posts_df.groupby(\"ParentId\"):\n",
    "    # Looping through the group of answers to the same post question to sort by Bytes and shift a Dataframe one \n",
    "    # one row down so could generate an easy zip like function using pd.concat and then generate diffs\n",
    "    try:\n",
    "        group.sort_values([\"ByteCount\"], ascending=False, inplace=True)\n",
    "        # Sorting by bytes to create diffs from a big program to small program for the same program, shifting one row down and concatenating pairs\n",
    "        # some questions have only one python answer so diff from empty string\n",
    "        if len(group)==1:\n",
    "            # if only one answer to the question then ignoring shift and proceeding to Diff from empty string.\n",
    "            group.rename(columns=lambda x: f'{x}_shifted', inplace=True)\n",
    "            group[\"Code\"] = \"\"\n",
    "            group[\"Id\"] = \"\"\n",
    "            df_concat = group\n",
    "        else:\n",
    "            shifted = group.shift().rename(columns=lambda x: f'{x}_shifted')\n",
    "            df_concat = pd.concat([shifted, group], axis=1).iloc[1:]\n",
    "        diffs = df_concat.apply(generate_diffs, axis=1)\n",
    "        # extracting useful columns alone to create a new Diff dataframe\n",
    "        code = df_concat[\"Code_shifted\"]\n",
    "        id = df_concat[\"Id_shifted\"]\n",
    "        text = df_concat[\"Text_shifted\"]\n",
    "        comments = df_concat[\"Comments_shifted\"]\n",
    "        no_of_rows = len(diffs)\n",
    "        diff_df = pd.concat([diff_df, pd.DataFrame({\"Diff\": diffs.values, \"Code\": code.values, \"ParentId\": np.full(no_of_rows, parentId), \"Id\":id.values, \"Text\": text.values, \"Comments\": comments})])   \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(code)\n",
    "        print(diffs) \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a filename mapping from Title string (removed special characters and any starting number) limited to 15 characters.\n",
    "filenames = dict(zip(parent_posts_df[\"Id\"], parent_posts_df[\"Title\"].str.replace('^\\d+|[^\\w\\s]+', '').str.replace('\\s+', '').str.strip().str.slice(stop=15) + '.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a tag mapping to be tested with LLMs for generating Git commit messages\n",
    "tags = dict(zip(parent_posts_df[\"Id\"], parent_posts_df[\"Tags\"].str.replace('[<>]', ' ').str.slice(stop=40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above generated dict to map the filename with questions\n",
    "diff_df[\"filename\"] = diff_df[\"ParentId\"].map(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing to generate prompts to query an LLM\n",
    "diff_df.reset_index(inplace=True)\n",
    "diff_df[\"Text\"].fillna(\" \", inplace=True)\n",
    "diff_df[\"Comments\"].fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating prompts to query LLM\n",
    "diff_df[\"prompt\"] = \"Generate a git commit message explaining, within 25 tokens, what the following git diff does with the help of description below \\n\" \\\n",
    "                        + diff_df[\"Text\"] + \" \\n \" + diff_df[\"Diff\"]+ \"\\n Commit Message:\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cohere's generate API to generate relevant Git Commit messages (Code Golf Problem) for finetuning Diff-Codgen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(\"Cohere API Key\")\n",
    "p = []\n",
    "index = 0\n",
    "for prompt in diff_df[\"prompt\"].values:\n",
    "    response = co.generate(  \n",
    "        model='xlarge',  \n",
    "        prompt = prompt,  \n",
    "        max_tokens=25,  \n",
    "        temperature=0.7,  \n",
    "        stop_sequences=[\"~--~\"])\n",
    "    p.append(response.generations[0].text)\n",
    "    print(f\"Completed: {index}\")\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning commit messages generated by the LLM\n",
    "diff_df[\"commitMessage\"] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file format for finetuning the Diff-Codegen Model\n",
    "final_format = ['<NME> {}\\n'\n",
    " '<BEF> {}\\n'\n",
    " '<MSG> {}\\n'\n",
    " '<DFF> {}'\n",
    " .format(row['filename'], row['Code'], row['commitMessage'], row['Diff']) for _, row in diff_df.iterrows()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
